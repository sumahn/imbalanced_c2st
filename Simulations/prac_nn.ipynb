{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "795c9101-f3f5-4a32-bb5c-f8c636d6611d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a23fcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch \n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "from utils import c2st_nn_fit, MatConvert, c2st_nn_power\n",
    "from models import myLDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69574400-d3d4-4811-b812-0ab74d610332",
   "metadata": {},
   "source": [
    "### Blob Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70d9a2fb-c5f0-4dd1-9080-a68d67f257c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_mx2_standard = np.array([[0.03, 0], [0, 0.03]])\n",
    "sigma_mx2 = np.zeros((9, 2, 2))\n",
    "for i in range(9):\n",
    "    sigma_mx2[i, :, :] = sigma_mx2_standard\n",
    "    if i < 4:\n",
    "        sigma_mx2[i, 0, 1] = -0.02 - 0.002*i\n",
    "        sigma_mx2[i, 1, 0] = -0.02 - 0.002*i\n",
    "    elif i > 4:\n",
    "        sigma_mx2[i, 0, 1] = -0.02 + 0.002*(i-5)\n",
    "        sigma_mx2[i, 1, 0] = -0.02 + 0.002*(i-5)\n",
    "        \n",
    "sigma_mx2[4, :, :] = sigma_mx2_standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "395da5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_blobs_Q(N0, N1, sigma_mx_2, rows=3, cols=3, rs=None):\n",
    "    \"\"\"Generate Blob-D for testing type-II error (or test power).\"\"\"\n",
    "    rs = check_random_state(rs)\n",
    "    mu = np.zeros(2)\n",
    "    sigma = np.eye(2) * 0.03\n",
    "    X = rs.multivariate_normal(mu, sigma, size=N0)\n",
    "    Y = rs.multivariate_normal(mu, np.eye(2), size=N1)\n",
    "    # assign to blobs\n",
    "    X[:, 0] += rs.randint(rows, size=N0)\n",
    "    X[:, 1] += rs.randint(cols, size=N0)\n",
    "    Y_row = rs.randint(rows, size=N1)\n",
    "    Y_col = rs.randint(cols, size=N1)\n",
    "    locs = [[0,0],[0,1],[0,2],[1,0],[1,1],[1,2],[2,0],[2,1],[2,2]]\n",
    "    for i in range(9):\n",
    "        corr_sigma = sigma_mx_2[i]\n",
    "        L = np.linalg.cholesky(corr_sigma)\n",
    "        ind = np.expand_dims((Y_row == locs[i][0]) & (Y_col == locs[i][1]), 1)\n",
    "        ind2 = np.concatenate((ind, ind), 1)\n",
    "        Y = np.where(ind2, np.matmul(Y,L) + locs[i], Y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9afb0064-4aad-4956-a1eb-aabb6c122d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup seeds \n",
    "np.random.seed(1203)\n",
    "torch.manual_seed(1203)\n",
    "torch.cuda.manual_seed(1203)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "is_cuda = True\n",
    "\n",
    "# Setup for all experiments \n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda:0\")\n",
    "alpha = 0.05 # test threshold \n",
    "ir_list = [1,3,5,7,9] # imbalance ratio\n",
    "x_in = 2 # number of neurons in the input layer (dimension of data)\n",
    "H = 50 # number of neurons in the hidden layer\n",
    "x_out = 2 # number of neurons in the output layer \n",
    "K = 1 # number of experiments\n",
    "n1 = 30 # size of minority samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f143d01e-c0ab-4451-bef6-760c26042f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "\t\t\t\tImbalance Ratio: 1\n",
      "\t\t\t\tExperiments1\n",
      "====================================================================================================\n",
      "Epoch 100, tau0: 0.3556, tau1: 0.5333, Asymp Power Loss: 1.8831, Power Stats Training: 1.8672\n",
      "Epoch 200, tau0: 0.3630, tau1: 0.4963, Asymp Power Loss: 2.2392, Power Stats Training: 2.3573\n",
      "Epoch 300, tau0: 0.4815, tau1: 0.4296, Asymp Power Loss: 1.3587, Power Stats Training: 1.4684\n",
      "Epoch 400, tau0: 0.4296, tau1: 0.4889, Asymp Power Loss: 1.3532, Power Stats Training: 1.3457\n",
      "Epoch 500, tau0: 0.5185, tau1: 0.3704, Asymp Power Loss: 1.8073, Power Stats Training: 1.8579\n",
      "Epoch 600, tau0: 0.4667, tau1: 0.4370, Asymp Power Loss: 1.6279, Power Stats Training: 1.5904\n",
      "Epoch 700, tau0: 0.4593, tau1: 0.4444, Asymp Power Loss: 1.5772, Power Stats Training: 1.5899\n",
      "Epoch 800, tau0: 0.5259, tau1: 0.3704, Asymp Power Loss: 1.7343, Power Stats Training: 1.7346\n",
      "Epoch 900, tau0: 0.5259, tau1: 0.3704, Asymp Power Loss: 1.7344, Power Stats Training: 1.7346\n",
      "Epoch 1000, tau0: 0.5259, tau1: 0.3704, Asymp Power Loss: 1.7345, Power Stats Training: 1.7346\n",
      "Epoch 1100, tau0: 0.5259, tau1: 0.3704, Asymp Power Loss: 1.7345, Power Stats Training: 1.7346\n",
      "Epoch 1200, tau0: 0.5259, tau1: 0.3704, Asymp Power Loss: 1.7345, Power Stats Training: 1.7346\n",
      "Epoch 1300, tau0: 0.5259, tau1: 0.3704, Asymp Power Loss: 1.7345, Power Stats Training: 1.7346\n",
      "Epoch 1400, tau0: 0.5259, tau1: 0.3704, Asymp Power Loss: 1.7345, Power Stats Training: 1.7346\n",
      "Epoch 1500, tau0: 0.5259, tau1: 0.3704, Asymp Power Loss: 1.7345, Power Stats Training: 1.7346\n",
      "Epoch 1600, tau0: 0.5259, tau1: 0.3704, Asymp Power Loss: 1.7345, Power Stats Training: 1.7346\n",
      "Epoch 1700, tau0: 0.5333, tau1: 0.3630, Asymp Power Loss: 1.7396, Power Stats Training: 1.7390\n",
      "Epoch 1800, tau0: 0.5333, tau1: 0.3630, Asymp Power Loss: 1.7386, Power Stats Training: 1.7390\n",
      "Epoch 1900, tau0: 0.3852, tau1: 0.5556, Asymp Power Loss: 1.0491, Power Stats Training: 0.9900\n",
      "Epoch 2000, tau0: 0.5630, tau1: 0.4222, Asymp Power Loss: 0.2594, Power Stats Training: 0.2459\n",
      "Epoch 2100, tau0: 0.5037, tau1: 0.4741, Asymp Power Loss: 0.3651, Power Stats Training: 0.3654\n",
      "Epoch 2200, tau0: 0.5037, tau1: 0.4741, Asymp Power Loss: 0.3654, Power Stats Training: 0.3654\n",
      "Epoch 2300, tau0: 0.5037, tau1: 0.4741, Asymp Power Loss: 0.3654, Power Stats Training: 0.3654\n",
      "Epoch 2400, tau0: 0.5037, tau1: 0.4741, Asymp Power Loss: 0.3654, Power Stats Training: 0.3654\n",
      "Epoch 2500, tau0: 0.5037, tau1: 0.4741, Asymp Power Loss: 0.3654, Power Stats Training: 0.3654\n",
      "Epoch 2600, tau0: 0.5037, tau1: 0.4741, Asymp Power Loss: 0.3654, Power Stats Training: 0.3654\n",
      "Epoch 2700, tau0: 0.5037, tau1: 0.4741, Asymp Power Loss: 0.3654, Power Stats Training: 0.3654\n",
      "Epoch 2800, tau0: 0.5037, tau1: 0.4741, Asymp Power Loss: 0.3654, Power Stats Training: 0.3654\n",
      "Epoch 2900, tau0: 0.5037, tau1: 0.4741, Asymp Power Loss: 0.3654, Power Stats Training: 0.3654\n",
      "Epoch 3000, tau0: 0.5037, tau1: 0.4741, Asymp Power Loss: 0.3654, Power Stats Training: 0.3654\n",
      "Epoch 3100, tau0: 0.5037, tau1: 0.4741, Asymp Power Loss: 0.3654, Power Stats Training: 0.3654\n",
      "Epoch 3200, tau0: 0.5037, tau1: 0.4741, Asymp Power Loss: 0.3654, Power Stats Training: 0.3654\n",
      "Epoch 3300, tau0: 0.5037, tau1: 0.4741, Asymp Power Loss: 0.3654, Power Stats Training: 0.3654\n",
      "Epoch 3400, tau0: 0.5037, tau1: 0.4741, Asymp Power Loss: 0.3654, Power Stats Training: 0.3654\n",
      "Epoch 3500, tau0: 0.5037, tau1: 0.4741, Asymp Power Loss: 0.3654, Power Stats Training: 0.3654\n",
      "Epoch 3600, tau0: 0.5037, tau1: 0.4741, Asymp Power Loss: 0.3654, Power Stats Training: 0.3654\n",
      "Epoch 3700, tau0: 0.5037, tau1: 0.4741, Asymp Power Loss: 0.3654, Power Stats Training: 0.3654\n",
      "Epoch 3800, tau0: 0.5037, tau1: 0.4741, Asymp Power Loss: 0.3654, Power Stats Training: 0.3654\n",
      "Epoch 3900, tau0: 0.5037, tau1: 0.4741, Asymp Power Loss: 0.3654, Power Stats Training: 0.3654\n",
      "Epoch 4000, tau0: 0.5037, tau1: 0.4741, Asymp Power Loss: 0.3654, Power Stats Training: 0.3654\n",
      "Power Stats Test: 0.9776259660720825 \t tau0_p: 0.5037037134170532 \t tau1_p: 0.43703702092170715\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\t\t\t\t   Testing\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "\t\t\t\tImbalance Ratio: 1\n",
      "\t\t\t\tExperiments2\n",
      "====================================================================================================\n",
      "Epoch 100, tau0: 0.5556, tau1: 0.3704, Asymp Power Loss: 1.0958, Power Stats Training: 1.2421\n",
      "Epoch 200, tau0: 0.6222, tau1: 0.3407, Asymp Power Loss: 0.6285, Power Stats Training: 0.6347\n",
      "Epoch 300, tau0: 0.6222, tau1: 0.3407, Asymp Power Loss: 0.6317, Power Stats Training: 0.6347\n",
      "Epoch 400, tau0: 0.6222, tau1: 0.3407, Asymp Power Loss: 0.6330, Power Stats Training: 0.6347\n",
      "Epoch 500, tau0: 0.6222, tau1: 0.3407, Asymp Power Loss: 0.6336, Power Stats Training: 0.6347\n",
      "Epoch 600, tau0: 0.6222, tau1: 0.3407, Asymp Power Loss: 0.6340, Power Stats Training: 0.6347\n",
      "Epoch 700, tau0: 0.6222, tau1: 0.3407, Asymp Power Loss: 0.6342, Power Stats Training: 0.6347\n",
      "Epoch 800, tau0: 0.6222, tau1: 0.3407, Asymp Power Loss: 0.6343, Power Stats Training: 0.6347\n",
      "Epoch 900, tau0: 0.6222, tau1: 0.3407, Asymp Power Loss: 0.6344, Power Stats Training: 0.6347\n",
      "Epoch 1000, tau0: 0.6222, tau1: 0.3407, Asymp Power Loss: 0.6344, Power Stats Training: 0.6347\n",
      "Epoch 1100, tau0: 0.6222, tau1: 0.3407, Asymp Power Loss: 0.6345, Power Stats Training: 0.6347\n",
      "Epoch 1200, tau0: 0.6222, tau1: 0.3407, Asymp Power Loss: 0.6345, Power Stats Training: 0.6347\n",
      "Epoch 1300, tau0: 0.6222, tau1: 0.3407, Asymp Power Loss: 0.6346, Power Stats Training: 0.6347\n",
      "Epoch 1400, tau0: 0.6222, tau1: 0.3407, Asymp Power Loss: 0.6346, Power Stats Training: 0.6347\n",
      "Epoch 1500, tau0: 0.6222, tau1: 0.3407, Asymp Power Loss: 0.6346, Power Stats Training: 0.6347\n",
      "Epoch 1600, tau0: 0.6222, tau1: 0.3407, Asymp Power Loss: 0.6346, Power Stats Training: 0.6347\n",
      "Epoch 1700, tau0: 0.6222, tau1: 0.3407, Asymp Power Loss: 0.6346, Power Stats Training: 0.6347\n",
      "Epoch 1800, tau0: 0.6222, tau1: 0.3407, Asymp Power Loss: 0.6346, Power Stats Training: 0.6347\n",
      "Epoch 1900, tau0: 0.6222, tau1: 0.3407, Asymp Power Loss: 0.6346, Power Stats Training: 0.6347\n",
      "Epoch 2000, tau0: 0.6222, tau1: 0.3407, Asymp Power Loss: 0.6346, Power Stats Training: 0.6347\n",
      "Epoch 2100, tau0: 0.6222, tau1: 0.3407, Asymp Power Loss: 0.6347, Power Stats Training: 0.6347\n",
      "Epoch 2200, tau0: 0.6222, tau1: 0.3407, Asymp Power Loss: 0.6347, Power Stats Training: 0.6347\n",
      "Epoch 2300, tau0: 0.6222, tau1: 0.3407, Asymp Power Loss: 0.6347, Power Stats Training: 0.6347\n",
      "Epoch 2400, tau0: 0.6222, tau1: 0.3407, Asymp Power Loss: 0.6347, Power Stats Training: 0.6347\n",
      "Epoch 2500, tau0: 0.6222, tau1: 0.3407, Asymp Power Loss: 0.6347, Power Stats Training: 0.6347\n",
      "Epoch 2600, tau0: 0.6222, tau1: 0.3407, Asymp Power Loss: 0.6347, Power Stats Training: 0.6347\n",
      "Epoch 2700, tau0: 0.6222, tau1: 0.3407, Asymp Power Loss: 0.6347, Power Stats Training: 0.6347\n",
      "Epoch 2800, tau0: 0.6222, tau1: 0.3407, Asymp Power Loss: 0.6347, Power Stats Training: 0.6347\n",
      "Epoch 2900, tau0: 0.6222, tau1: 0.3407, Asymp Power Loss: 0.6347, Power Stats Training: 0.6347\n",
      "Epoch 3000, tau0: 0.6222, tau1: 0.3407, Asymp Power Loss: 0.6347, Power Stats Training: 0.6347\n",
      "Epoch 3100, tau0: 0.6222, tau1: 0.3407, Asymp Power Loss: 0.6347, Power Stats Training: 0.6347\n",
      "Epoch 3200, tau0: 0.6222, tau1: 0.3407, Asymp Power Loss: 0.6347, Power Stats Training: 0.6347\n",
      "Epoch 3300, tau0: 0.6222, tau1: 0.3407, Asymp Power Loss: 0.6347, Power Stats Training: 0.6347\n",
      "Epoch 3400, tau0: 0.6222, tau1: 0.3407, Asymp Power Loss: 0.6347, Power Stats Training: 0.6347\n",
      "Epoch 3500, tau0: 0.6222, tau1: 0.3407, Asymp Power Loss: 0.6347, Power Stats Training: 0.6347\n",
      "Epoch 3600, tau0: 0.6222, tau1: 0.3407, Asymp Power Loss: 0.6347, Power Stats Training: 0.6347\n",
      "Epoch 3700, tau0: 0.6222, tau1: 0.3407, Asymp Power Loss: 0.6347, Power Stats Training: 0.6347\n",
      "Epoch 3800, tau0: 0.6222, tau1: 0.3407, Asymp Power Loss: 0.6347, Power Stats Training: 0.6347\n",
      "Epoch 3900, tau0: 0.6222, tau1: 0.3407, Asymp Power Loss: 0.6347, Power Stats Training: 0.6347\n",
      "Epoch 4000, tau0: 0.6222, tau1: 0.3407, Asymp Power Loss: 0.6347, Power Stats Training: 0.6347\n",
      "Power Stats Test: -1.1901472806930542 \t tau0_p: 0.7259259223937988 \t tau1_p: 0.34074074029922485\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\t\t\t\t   Testing\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "\t\t\t\tImbalance Ratio: 1\n",
      "\t\t\t\tExperiments3\n",
      "====================================================================================================\n",
      "Epoch 100, tau0: 0.1407, tau1: 0.7556, Asymp Power Loss: 2.0209, Power Stats Training: 2.1796\n",
      "Epoch 200, tau0: 0.1407, tau1: 0.7556, Asymp Power Loss: 2.0870, Power Stats Training: 2.1796\n",
      "Epoch 300, tau0: 0.1407, tau1: 0.7556, Asymp Power Loss: 2.0758, Power Stats Training: 2.1796\n",
      "Epoch 400, tau0: 0.1037, tau1: 0.8222, Asymp Power Loss: 1.6962, Power Stats Training: 1.7600\n",
      "Epoch 500, tau0: 0.1037, tau1: 0.8222, Asymp Power Loss: 1.7313, Power Stats Training: 1.7600\n",
      "Epoch 600, tau0: 0.1037, tau1: 0.8222, Asymp Power Loss: 1.7445, Power Stats Training: 1.7600\n",
      "Epoch 700, tau0: 0.1037, tau1: 0.8222, Asymp Power Loss: 1.7508, Power Stats Training: 1.7600\n",
      "Epoch 800, tau0: 0.1037, tau1: 0.8222, Asymp Power Loss: 1.7541, Power Stats Training: 1.7600\n",
      "Epoch 900, tau0: 0.1037, tau1: 0.8222, Asymp Power Loss: 1.7560, Power Stats Training: 1.7600\n",
      "Epoch 1000, tau0: 0.1037, tau1: 0.8222, Asymp Power Loss: 1.7571, Power Stats Training: 1.7600\n",
      "Epoch 1100, tau0: 0.1037, tau1: 0.8222, Asymp Power Loss: 1.7578, Power Stats Training: 1.7600\n",
      "Epoch 1200, tau0: 0.1037, tau1: 0.8222, Asymp Power Loss: 1.7583, Power Stats Training: 1.7600\n",
      "Epoch 1300, tau0: 0.1037, tau1: 0.8222, Asymp Power Loss: 1.7587, Power Stats Training: 1.7600\n",
      "Epoch 1400, tau0: 0.1037, tau1: 0.8222, Asymp Power Loss: 1.7590, Power Stats Training: 1.7600\n",
      "Epoch 1500, tau0: 0.1037, tau1: 0.8222, Asymp Power Loss: 1.7591, Power Stats Training: 1.7600\n",
      "Epoch 1600, tau0: 0.1037, tau1: 0.8222, Asymp Power Loss: 1.7593, Power Stats Training: 1.7600\n",
      "Epoch 1700, tau0: 0.1037, tau1: 0.8222, Asymp Power Loss: 1.7594, Power Stats Training: 1.7600\n",
      "Epoch 1800, tau0: 0.1037, tau1: 0.8222, Asymp Power Loss: 1.7595, Power Stats Training: 1.7600\n",
      "Epoch 1900, tau0: 0.1037, tau1: 0.8222, Asymp Power Loss: 1.7596, Power Stats Training: 1.7600\n",
      "Epoch 2000, tau0: 0.1037, tau1: 0.8222, Asymp Power Loss: 1.7596, Power Stats Training: 1.7600\n",
      "Epoch 2100, tau0: 0.1037, tau1: 0.8222, Asymp Power Loss: 1.7597, Power Stats Training: 1.7600\n",
      "Epoch 2200, tau0: 0.1037, tau1: 0.8222, Asymp Power Loss: 1.7597, Power Stats Training: 1.7600\n",
      "Epoch 2300, tau0: 0.1037, tau1: 0.8222, Asymp Power Loss: 1.7598, Power Stats Training: 1.7600\n",
      "Epoch 2400, tau0: 0.1037, tau1: 0.8222, Asymp Power Loss: 1.7598, Power Stats Training: 1.7600\n",
      "Epoch 2500, tau0: 0.1037, tau1: 0.8222, Asymp Power Loss: 1.7598, Power Stats Training: 1.7600\n",
      "Epoch 2600, tau0: 0.1037, tau1: 0.8222, Asymp Power Loss: 1.7598, Power Stats Training: 1.7600\n",
      "Epoch 2700, tau0: 0.1037, tau1: 0.8222, Asymp Power Loss: 1.7599, Power Stats Training: 1.7600\n",
      "Epoch 2800, tau0: 0.1037, tau1: 0.8222, Asymp Power Loss: 1.7599, Power Stats Training: 1.7600\n",
      "Epoch 2900, tau0: 0.1037, tau1: 0.8222, Asymp Power Loss: 1.7599, Power Stats Training: 1.7600\n",
      "Epoch 3000, tau0: 0.1037, tau1: 0.8222, Asymp Power Loss: 1.7599, Power Stats Training: 1.7600\n",
      "Epoch 3100, tau0: 0.1037, tau1: 0.8222, Asymp Power Loss: 1.7599, Power Stats Training: 1.7600\n",
      "Epoch 3200, tau0: 0.1037, tau1: 0.8222, Asymp Power Loss: 1.7599, Power Stats Training: 1.7600\n",
      "Epoch 3300, tau0: 0.1037, tau1: 0.8222, Asymp Power Loss: 1.7599, Power Stats Training: 1.7600\n",
      "Epoch 3400, tau0: 0.1037, tau1: 0.8222, Asymp Power Loss: 1.7600, Power Stats Training: 1.7600\n",
      "Epoch 3500, tau0: 0.1037, tau1: 0.8222, Asymp Power Loss: 1.7600, Power Stats Training: 1.7600\n",
      "Epoch 3600, tau0: 0.1037, tau1: 0.8222, Asymp Power Loss: 1.7600, Power Stats Training: 1.7600\n",
      "Epoch 3700, tau0: 0.1037, tau1: 0.8222, Asymp Power Loss: 1.7600, Power Stats Training: 1.7600\n",
      "Epoch 3800, tau0: 0.1037, tau1: 0.8222, Asymp Power Loss: 1.7600, Power Stats Training: 1.7600\n",
      "Epoch 3900, tau0: 0.1037, tau1: 0.8222, Asymp Power Loss: 1.7600, Power Stats Training: 1.7600\n",
      "Epoch 4000, tau0: 0.1037, tau1: 0.8222, Asymp Power Loss: 1.7600, Power Stats Training: 1.7600\n",
      "Power Stats Test: 0.5577274560928345 \t tau0_p: 0.1111111119389534 \t tau1_p: 0.8666666746139526\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\t\t\t\t   Testing\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2684610/3199828002.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mstats_p_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstats_p_nn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;31m# power\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mpwr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats_p_nn\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mppf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0mp_powers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpwr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Asymptotic power: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpwr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/work/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    954\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 956\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    957\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "# For each n in n_list, train deep kernel and run two-sample test\n",
    "stats_a_dict = {}\n",
    "stats_d_dict = {}\n",
    "stats_p_dict = {}\n",
    "\n",
    "acc_opt_cuts = []\n",
    "pwr_opt_cuts = []\n",
    "\n",
    "a_powers = {}\n",
    "d_powers = {}\n",
    "p_powers = {}\n",
    "\n",
    "for r in ir_list:\n",
    "    np.random.seed(1203)\n",
    "    N1 = 9 * n1 * r\n",
    "    N2 = 9 * n1\n",
    "    batch_size = N1 + N2 # not using batch \n",
    "    n_epoch_c2st = 4000\n",
    "    \n",
    "    # Repeat experiments K times (K = 10) and report average test powers\n",
    "    stats_p_nn = []\n",
    "    \n",
    "    for kk in range(3):\n",
    "        print('='*100)\n",
    "        print(\"\\t\\t\\t\\tImbalance Ratio: {}\".format(int(N1/N2)))\n",
    "        print(f\"\\t\\t\\t\\tExperiments{kk+1}\")\n",
    "        print('='*100)\n",
    "        # Generate Blob-D\n",
    "        np.random.seed(seed=112*kk + 1 + N1)\n",
    "        s1, s2 = sample_blobs_Q(N1, N2, sigma_mx2)\n",
    "        \n",
    "        S = np.concatenate((s1,s2), axis=0)\n",
    "        S = MatConvert(S, device, dtype)\n",
    "        \n",
    "        # Train C2ST-Power\n",
    "        np.random.seed(seed=1203)\n",
    "        torch.manual_seed(1203)\n",
    "        torch.cuda.manual_seed(1203)\n",
    "        y = (torch.cat((torch.zeros(N1, 1), torch.ones(N2, 1)), 0)).squeeze(1).to(device, dtype).long()\n",
    "        \n",
    "        pred, tau0, tau1, stat_c2st = c2st_nn_power(S, y, x_in, H, x_out, 0.005, n_epoch_c2st, batch_size, device, dtype)\n",
    "        print(\"-\"*100)\n",
    "        print('-'*100)\n",
    "        print(\"\\t\\t\\t\\t   Testing\")\n",
    "        print('-'*100)\n",
    "        print('-'*100)\n",
    "        \n",
    "        # statistics\n",
    "        stats_p_nn.append(stat_c2st)\n",
    "\n",
    "    stats_p_dict[r] = stats_p_nn\n",
    "    # power\n",
    "    pwr = np.mean((stats_p_nn > (1-norm.ppf(alpha))))\n",
    "    p_powers[r] = pwr\n",
    "    print(\"Asymptotic power: {}\".format(pwr))\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a991a673-22fe-4dc5-94ba-9d9f91b51442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define the neural network architecture\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(2, 4)\n",
    "        self.layer2 = nn.Linear(4, 8)\n",
    "        self.layer3 = nn.Linear(8, 16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc_layer = nn.Linear(16, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.fc_layer(x)\n",
    "        return x\n",
    "\n",
    "# Define the training loop\n",
    "def train(model, optimizer, criterion, train_loader, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print('Epoch [%d], loss: %.3f' % (epoch + 1, running_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88524fa-5a9d-4ad7-9d09-09222bc0b838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "X_train, y_train = train_data.iloc[:, :-1].values, train_data.iloc[:, -1].values\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=12, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3df95c-23d1-4ca5-b503-158967a25fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, optimizer, and loss function\n",
    "model = NeuralNetwork()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d7e06c-975a-402a-84f6-dd143ac1313a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train(model, optimizer, criterion, train_loader, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79fd599-e89b-40f6-a354-8b04026f7a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_te = torch.tensor(X_te.values, dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7423b764-84b7-40bf-8dc3-8f060efb1f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d643ce-881b-4d1d-a648-5d31a617869c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    pred = model(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a549045f-8a38-4383-9b75-9ed70707fd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a59ba6e-73df-4317-9395-2f8352f11f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n0 >> n1 \n",
    "val_n0 = np.sum(y_te == 0) # majority sample\n",
    "val_n1 = np.sum(y_te == 1) # minority sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f29f88b-9017-4967-ab8e-732fae2624e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train neural network classifier for optimizing accuracy vs power\n",
    "val_nn = MLPClassifier(hidden_layer_sizes=(3,2), max_iter=10000, verbose=True, early_stopping = True, validation_fraction=0.2, learning_rate='adaptive')\n",
    "val_nn.fit(X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba2c5bc-2c82-4f97-aede-e540657dc124",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_probs = val_nn.predict_proba(X_te)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73cdb5e-843b-4b83-af92-34969eb2a18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = np.zeros(len(cutvals))\n",
    "power = np.zeros(len(cutvals))\n",
    "for j in range(len(cutvals)):\n",
    "    k = cutvals[j]\n",
    "    tmp = (probs > k).astype(int)\n",
    "    acc[j] = np.sum(tmp == y_te) / len(y_te)\n",
    "    tau0 = np.mean(tmp[y_te == 0] == 1) # P_{X ~ q}(h(X) = 1 | Y = 0)\n",
    "    tau1 = np.mean(tmp[y_te == 1] == 0) # P_{X ~ p}(h(X) = 0 | Y = 1)\n",
    "    if int(tau1) == 1:\n",
    "        power[j] = 0\n",
    "    else:\n",
    "        power[j] = (1 - tau0 - tau1) / np.sqrt(tau0 * (1 - tau0) / n0 + tau1 * (1 - tau1) / n1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f7bde7-ae46-4d1a-a1d3-f511e8f2b143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal cutoff\n",
    "acc_cut_opt = cutvals[np.argmax(acc)]\n",
    "power_cut_opt = cutvals[np.argmax(power)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c36f7a8-adc5-4d6f-9430-0fb53be3ed96",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31615fe0-5ebf-4175-b556-3848cda45f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test \n",
    "test_n0 = np.sum(y_test == 0) # majority\n",
    "test_n1 = np.sum(y_test == 1) # minority \n",
    "\n",
    "test_nn = MLPClassifier(hidden_layer_sizes=(3,2), max_iter=10000, verbose=True, early_stopping = True, validation_fraction=0.2, learning_rate='adaptive')\n",
    "test_nn.fit(X_train, y_train)\n",
    "test_probs = test_nn.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dcd526-f1b4-4f0e-8397-7f72c46dfe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "y_acc_pred = (test_probs > acc_cut_opt).astype(int)\n",
    "y_power_pred = (test_probs > power_cut_opt).astype(int)\n",
    "y_default_pred = (test_probs > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244afeca-784e-4e8f-9ed5-0c11b4da7f23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(y_acc_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e129dcd7-8ffc-446d-98e4-d6ae2071c8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(y_power_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fcda89-e1ca-452d-891c-cc62b7a1540c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(y_default_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ca4e94-7ece-4797-86c9-c281fad7f1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train neural network classifier for optimizing accuracy vs power\n",
    "val_nn = MLPClassifier(hidden_layer_sizes=(5,2), max_iter=10000)\n",
    "val_nn.fit(X_tr, y_tr)\n",
    "probs = val_nn.predict_proba(X_te)[:, 1]\n",
    "acc = np.zeros(len(cutvals))\n",
    "power = np.zeros(len(cutvals))\n",
    "for j in range(len(cutvals)):\n",
    "    k = cutvals[j]\n",
    "    tmp = (probs > k).astype(int)\n",
    "    acc[j] = np.sum(tmp == y_te) / len(y_te)\n",
    "    tau0 = np.mean(tmp[y_te == 0] == 1) # P_{X ~ q}(h(X) = 1 | Y = 0)\n",
    "    tau1 = np.mean(tmp[y_te == 1] == 0) # P_{X ~ p}(h(X) = 0 | Y = 1)\n",
    "    if int(tau1) == 1:\n",
    "        power[j] = 0\n",
    "    else:\n",
    "        power[j] = (1 - tau0 - tau1) / np.sqrt(tau0 * (1 - tau0) / n0 + tau1 * (1 - tau1) / n1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# prediction\n",
    "y_acc_pred = (test_probs > acc_cut_opt).astype(int)\n",
    "y_power_pred = (test_probs > power_cut_opt).astype(int)\n",
    "y_default_pred = (test_probs > 0.5).astype(int)\n",
    "\n",
    "tau0_a = np.mean(y_acc_pred[y_test == 0] == 1) # P_{X ~ q}(h(X) = 1)\n",
    "tau1_a = np.mean(y_acc_pred[y_test == 1] == 0) # P_{X ~ q}(h(X) = 0)\n",
    "stat_a[i] = (1 - tau0_a - tau1_a) / np.sqrt((tau0_a*(1-tau0_a)/test_n0) + (tau1_a*(1-tau1_a)/test_n1))\n",
    "\n",
    "tau0_d = np.mean(y_default_pred[y_test == 0] == 1) # P_{X ~ q}(h(X) = 1)\n",
    "tau1_d = np.mean(y_default_pred[y_test == 1] == 0) # P_{X ~ q}(h(X) = 0)\n",
    "stat_d[i] = (1 - tau0_d - tau1_d) / np.sqrt((tau0_d*(1-tau0_d)/test_n0) + (tau1_d*(1-tau1_d)/test_n1))\n",
    "\n",
    "tau0_p = np.mean(y_power_pred[y_test == 0] == 1) # P_{X ~ q}(h(X) = 1)\n",
    "tau1_p = np.mean(y_power_pred[y_test == 1] == 0) # P_{X ~ q}(h(X) = 0)\n",
    "stat_p[i] = (1 - tau0_p - tau1_p) / np.sqrt((tau0_p*(1-tau0_p)/test_n0) + (tau1_p*(1-tau1_p)/test_n1))\n",
    "\n",
    "print(\"=\"* 30)\n",
    "print(\"acc cut opt: \", acc_cut_opt)\n",
    "print(\"pwr cut opt: \", power_cut_opt)\n",
    "print(\"Stat of acc: \" , stat_a[i])\n",
    "print(\"Stat of default: \", stat_d[i])\n",
    "print(\"Stat of power: \", stat_p[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87780c25-65af-4cdc-bc56-3c90ae3bdcf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "191ec752323c442db01b072a853b2dd7b367cc1eb2408ac8b5045f825f91cc90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
