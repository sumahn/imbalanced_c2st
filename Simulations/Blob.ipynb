{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63c1723d-b1e2-40ec-a6de-3f716ee13103",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c3e8153",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e010cb80-1a23-4384-b841-106b27d208ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('/home/oldrain123/imbalanced_c2st/')\n",
    "\n",
    "import torch \n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "from model import LeNet\n",
    "from utils import MatConvert, GenerateData"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "424bd277-9088-46df-a5e8-d60f464f4c08",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b116ad3e-f404-478d-96c2-1fbb151ad802",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_mx2_standard = np.array([[0.03, 0], [0, 0.03]])\n",
    "sigma_mx2 = np.zeros((9, 2, 2))\n",
    "for i in range(9):\n",
    "    sigma_mx2[i, :, :] = sigma_mx2_standard\n",
    "    if i < 4:\n",
    "        sigma_mx2[i, 0, 1] = -0.02 - 0.002*i\n",
    "        sigma_mx2[i, 1, 0] = -0.02 - 0.002*i\n",
    "    elif i > 4:\n",
    "        sigma_mx2[i, 0, 1] = -0.02 + 0.002*(i-5)\n",
    "        sigma_mx2[i, 1, 0] = -0.02 + 0.002*(i-5)\n",
    "        \n",
    "sigma_mx2[4, :, :] = sigma_mx2_standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dfb3293-9e6a-4efc-8fd0-49349a2cf4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_blobs_Q(N0, N1, sigma_mx_2, rows=3, cols=3, rs=None):\n",
    "    \"\"\"Generate Blob-D for testing type-II error (or test power).\"\"\"\n",
    "    rs = check_random_state(rs)\n",
    "    mu = np.zeros(2)\n",
    "    sigma = np.eye(2) * 0.03\n",
    "    X = rs.multivariate_normal(mu, sigma, size=N0)\n",
    "    Y = rs.multivariate_normal(mu, np.eye(2), size=N1)\n",
    "    # assign to blobs\n",
    "    X[:, 0] += rs.randint(rows, size=N0)\n",
    "    X[:, 1] += rs.randint(cols, size=N0)\n",
    "    Y_row = rs.randint(rows, size=N1)\n",
    "    Y_col = rs.randint(cols, size=N1)\n",
    "    locs = [[0,0],[0,1],[0,2],[1,0],[1,1],[1,2],[2,0],[2,1],[2,2]]\n",
    "    for i in range(9):\n",
    "        corr_sigma = sigma_mx_2[i]\n",
    "        L = np.linalg.cholesky(corr_sigma)\n",
    "        ind = np.expand_dims((Y_row == locs[i][0]) & (Y_col == locs[i][1]), 1)\n",
    "        ind2 = np.concatenate((ind, ind), 1)\n",
    "        Y = np.where(ind2, np.matmul(Y,L) + locs[i], Y)\n",
    "    return X, Y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34c5000b",
   "metadata": {},
   "source": [
    "### Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be6c86b7-3907-4ddc-ae55-dde59ac81cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup seeds \n",
    "np.random.seed(1203)\n",
    "torch.manual_seed(1203)\n",
    "torch.cuda.manual_seed(1203)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "is_cuda = True\n",
    "\n",
    "# Setup for all experiments \n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda:0\")\n",
    "alpha = 0.05 # test threshold \n",
    "ir_list = [1,3,5,7,9] # imbalance ratio\n",
    "x_in = 2 # number of neurons in the input layer (dimension of data)\n",
    "H = 50 # number of neurons in the hidden layer\n",
    "x_out = 2 # number of neurons in the output layer \n",
    "K = 1 # number of experiments\n",
    "n1 = 100 # size of minority samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b80e184b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet(x_in, H, x_out, device, dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23d18d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1, s2 = sample_blobs_Q(100, 100, sigma_mx2)\n",
    "        \n",
    "S = np.concatenate((s1,s2), axis=0)\n",
    "S = MatConvert(S, device, dtype)\n",
    "y = (torch.cat((torch.zeros(100, 1), torch.ones(100, 1)), 0)).squeeze(1).to(device, dtype).long()\n",
    "# generate train and test indices\n",
    "y_np = y.to('cpu').numpy()\n",
    "tmp_idx, test_idx, tmp_y, test_y = train_test_split(np.arange(len(y_np)), y_np, test_size=0.5, stratify=y_np)\n",
    "\n",
    "# generate validation indices \n",
    "train_idx, val_idx, train_y, val_y = train_test_split(tmp_idx, y_np[tmp_idx], test_size=0.5, stratify=y_np[tmp_idx])\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(S[train_idx], y[train_idx])\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size = 128, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "935eb26b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeNet(\n",
       "  (latent): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
       "    (1): Softplus(beta=1, threshold=20)\n",
       "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
       "    (3): Softplus(beta=1, threshold=20)\n",
       "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
       "    (5): Softplus(beta=1, threshold=20)\n",
       "    (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataloader, lr=0.01, n_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc2f84a-200f-45a2-a339-c74690593acf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For each n in n_list, train deep kernel and run two-sample test\n",
    "stats_a_dict = {}\n",
    "stats_d_dict = {}\n",
    "stats_p_dict = {}\n",
    "\n",
    "acc_opt_cuts = []\n",
    "pwr_opt_cuts = []\n",
    "\n",
    "a_powers = {}\n",
    "d_powers = {}\n",
    "p_powers = {}\n",
    "\n",
    "for r in ir_list:\n",
    "    np.random.seed(1203)\n",
    "    N1 = 9 * n1 * r\n",
    "    N2 = 9 * n1\n",
    "    batch_size = N1 + N2 # not using batch \n",
    "    n_epoch_c2st = 1200\n",
    "    \n",
    "    # Repeat experiments K times (K = 10) and report average test powers\n",
    "    stats_a_nn = []\n",
    "    stats_d_nn = []\n",
    "    stats_p_nn = []\n",
    "    \n",
    "    for kk in range(50):\n",
    "        print('='*100)\n",
    "        print(\"\\t\\t\\t\\tImbalance Ratio: {}\".format(int(N1/N2)))\n",
    "        print(f\"\\t\\t\\t\\tExperiments{kk+1}\")\n",
    "        print('='*100)\n",
    "        # Generate Blob-D\n",
    "        np.random.seed(seed=112*kk + 1 + N1)\n",
    "        s1, s2 = sample_blobs_Q(N1, N2, sigma_mx2)\n",
    "        \n",
    "        S = np.concatenate((s1,s2), axis=0)\n",
    "        S = MatConvert(S, device, dtype)\n",
    "        \n",
    "        # Train C2ST-L\n",
    "        np.random.seed(seed=1203)\n",
    "        torch.manual_seed(1203)\n",
    "        torch.cuda.manual_seed(1203)\n",
    "        y = (torch.cat((torch.zeros(N1, 1), torch.ones(N2, 1)), 0)).squeeze(1).to(device, dtype).long()\n",
    "        cutvals = np.arange(start=0.001, stop=0.999, step = 0.001)\n",
    "        acc, pwr, acc_opt_cut, pwr_opt_cut = c2st_nn_fit(S, y, x_in, H, x_out, cutvals, 0.01, n_epoch_c2st, batch_size, device, dtype, validation=True)\n",
    "        acc_opt_cuts.append(acc_opt_cut)\n",
    "        pwr_opt_cuts.append(pwr_opt_cut)\n",
    "        print(\"-\"*100)\n",
    "        print('-'*100)\n",
    "        print(\"\\t\\t\\t\\t   Testing\")\n",
    "        print('-'*100)\n",
    "        print('-'*100)\n",
    "        tau0_a, tau1_a, tau0_p, tau1_p, stat_a, stat_d, stat_p = c2st_nn_fit(S, y, x_in, H, x_out, cutvals, 0.01, n_epoch_c2st*2, batch_size, device, dtype, validation=False, acc_opt_cut=acc_opt_cut, pwr_opt_cut=pwr_opt_cut)\n",
    "        \n",
    "        # statistics\n",
    "        stats_a_nn.append(stat_a)\n",
    "        stats_d_nn.append(stat_d)\n",
    "        stats_p_nn.append(stat_p)\n",
    "\n",
    "    stats_a_dict[r] = stats_a_nn\n",
    "    stats_d_dict[r] = stats_d_nn\n",
    "    stats_p_dict[r] = stats_p_nn\n",
    "    # power\n",
    "    cr = norm.ppf(1-alpha)\n",
    "    a_pwr = np.mean((stats_a_nn > (1-norm.ppf(alpha))))\n",
    "    d_pwr = np.mean((stats_d_nn > (1-norm.ppf(alpha))))\n",
    "    p_pwr = np.mean((stats_p_nn > (1-norm.ppf(alpha))))\n",
    "    \n",
    "    a_powers[r] = a_pwr\n",
    "    d_powers[r] = d_pwr\n",
    "    p_powers[r] = p_pwr\n",
    "    print(\"Accuracy Power: {}\".format(a_pwr), '\\t', \"Default Power: {}\".format(d_pwr), '\\t', \"Power Power: {}\".format(p_pwr))\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f81420",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_powers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8962976d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_a = []\n",
    "data_p = []\n",
    "data_d = []\n",
    "for r in ir_list:\n",
    "    data_a.append({'imbalance_ratio': r, 'power': a_powers[r]})\n",
    "    data_p.append({'imbalance_ratio': r, 'power': p_powers[r]})\n",
    "    data_d.append({'imbalance_ratio': r, 'power': d_powers[r]})\n",
    "    \n",
    "        \n",
    "df_a = pd.DataFrame(data_a)\n",
    "df_p = pd.DataFrame(data_p)\n",
    "df_d = pd.DataFrame(data_d)\n",
    "# Create the plotNo\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.lineplot(data=df_a, x=\"imbalance_ratio\", y=\"power\", label='Max Acc')\n",
    "sns.lineplot(data=df_p, x=\"imbalance_ratio\", y=\"power\", label='Max Pwr')\n",
    "sns.lineplot(data=df_d, x=\"imbalance_ratio\", y=\"power\", label='Default')\n",
    "\n",
    "# Add axis labels and a title\n",
    "plt.xlabel(\"Imbalance Ratio\")\n",
    "plt.ylabel(\"Power\")\n",
    "plt.title(\"Blob Dataset\")\n",
    "plt.legend()\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d57403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each n in n_list, train deep kernel and run two-sample test\n",
    "stats_a_dict = {}\n",
    "stats_d_dict = {}\n",
    "stats_p_dict = {}\n",
    "\n",
    "acc_opt_cuts = []\n",
    "pwr_opt_cuts = []\n",
    "\n",
    "a_powers = {}\n",
    "d_powers = {}\n",
    "p_powers = {}\n",
    "\n",
    "for r in ir_list:\n",
    "    np.random.seed(1203)\n",
    "    N1 = n1 * r\n",
    "    N2 = n1\n",
    "    batch_size = N1 + N2 # not using batch \n",
    "    n_epoch_c2st = 1000\n",
    "    \n",
    "    # Repeat experiments K times (K = 10) and report average test powers\n",
    "    stats_a = []\n",
    "    stats_d = []\n",
    "    stats_p = []\n",
    "    \n",
    "    for kk in range(K):\n",
    "        print('='*100)\n",
    "        print(\"\\t\\t\\t\\tImbalance Ratio: {}\".format(int(N1/N2)))\n",
    "        print(f\"\\t\\t\\t\\tExperiments{kk+1}\")\n",
    "        print('='*100)\n",
    "        # Generate Blob-D\n",
    "        np.random.seed(seed=112*kk + 1 + N1)\n",
    "        s1, s2 = sample_blobs_Q(N1, N2, sigma_mx2)\n",
    "        \n",
    "        if kk == 0:\n",
    "            s1_o = s1\n",
    "            s2_o = s2\n",
    "        \n",
    "        S = np.concatenate((s1,s2), axis=0)\n",
    "        S = MatConvert(S, device, dtype)\n",
    "        \n",
    "        # Train C2ST-L\n",
    "        np.random.seed(seed=1203)\n",
    "        torch.manual_seed(1203)\n",
    "        torch.cuda.manual_seed(1203)\n",
    "        y = (torch.cat((torch.zeros(N1, 1), torch.ones(N2, 1)), 0)).squeeze(1).to(device, dtype).long()\n",
    "        cutvals = np.linspace(0.001, 0.999, 1000)\n",
    "        pred, tau0, tau1, stat_c2st = c2st_nn_power(S, y, x_in, H, x_out, 0.01, n_epoch_c2st, batch_size, device, dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cac6a7-876d-4a02-a1c6-c4ac62799b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each n in n_list, train deep kernel and run two-sample test\n",
    "\n",
    "stats = {}\n",
    "powers = []\n",
    "for n in n_list:\n",
    "    np.random.seed(12203)\n",
    "    torch.manual_seed(12203)\n",
    "    torch.cuda.manual_seed(12203)\n",
    "    N1 = 30 * n\n",
    "    N2 = 300\n",
    "    results = np.zeros([5, K])\n",
    "    J_star_adp = np.zeros([K, n_epoch])\n",
    "    batch_size = N1 + N2 \n",
    "    n_epoch_c2st = 500\n",
    "    \n",
    "    # Repeat experiments K times (K = 10) and report average test power \n",
    "    stat = []\n",
    "    for kk in range(K):\n",
    "        print(\"Imbalance Ratio: {}\".format(N1/N2))\n",
    "        print(f\"{kk+1} Experiments...\")\n",
    "        \n",
    "        # Generate Blob-D\n",
    "        np.random.seed(seed=112*kk + 1 + n)\n",
    "        s1, s2 = sample_blobs_Q(N1, N2, sigma_mx2)\n",
    "        \n",
    "        if kk == 0:\n",
    "            s1_o = s1\n",
    "            s2_o = s2\n",
    "        \n",
    "        S = np.concatenate((s1,s2), axis=0)\n",
    "        S = MatConvert(S, device, dtype)\n",
    "        \n",
    "        # Train C2ST-L\n",
    "        np.random.seed(seed=12203)\n",
    "        torch.manual_seed(12203)\n",
    "        torch.cuda.manual_seed(12203)\n",
    "        y = (torch.cat((torch.zeros(N1, 1), torch.ones(N2, 1)), 0)).squeeze(1).to(device, dtype).long()\n",
    "        pred, tau0, tau1, stat_c2st_l, model_c2st_l, w_c2st_l, b_c2st_l = C2ST_NN_power(S, y, N1, x_in, H, x_out, 0.0005, \n",
    "                                                                          n_epoch_c2st, batch_size, device, dtype)\n",
    "        # statistics\n",
    "        stat.append(stat_c2st_l)\n",
    "        print(pred.unique(return_counts=True))\n",
    "        print(f\"tau0: {tau0} \\t tau1: {tau1} \", end='\\t')\n",
    "        print(\"Stats: {}\".format(stat_c2st_l))\n",
    "        stats[n] = stat\n",
    "        \n",
    "    # power\n",
    "    pwr = len([i for i in stats[n] if i > (1-norm.ppf(alpha))]) / len(stats[n])\n",
    "    powers.append(pwr)\n",
    "    print(\"Power: {}\".format(pwr))\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eaad4c-d1e0-4180-8b8d-e59c71752385",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5bd4db-270b-45cb-96d6-3f95c5764b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901bfb54-fe75-4f3c-bb69-17a959422e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_c2st_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6827a62-0b4f-4121-843d-5df6fc5340e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.unique(return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75660d39-8982-409b-9bee-bc0fe74a7699",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1999ee15-f2fa-4651-8a1b-86f3023a7f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "df = sample_blobs_Q(500, 500, sigma_mx2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f940a2-5f2b-4f00-a3c2-474c9712f948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "data = sample_blobs_Q(n0=500, n1=500, sigma_mx_2=sigma_mx2)\n",
    "\n",
    "# Split data into training and test sets\n",
    "train_data = data.sample(frac=0.8, random_state=42)\n",
    "test_data = data.drop(train_data.index)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "train_x = torch.tensor(train_data.iloc[:, :-1].values, dtype=torch.float32)\n",
    "train_y = torch.tensor(train_data.iloc[:, -1].values, dtype=torch.int64)\n",
    "test_x = torch.tensor(test_data.iloc[:, :-1].values, dtype=torch.float32)\n",
    "test_y = torch.tensor(test_data.iloc[:, -1].values, dtype=torch.int64)\n",
    "\n",
    "# Define the LeNet model\n",
    "model = LeNet()\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    # Shuffle the training data\n",
    "    indices = torch.randperm(train_x.shape[0])\n",
    "    train_x = train_x[indices]\n",
    "    train_y = train_y[indices]\n",
    "    \n",
    "    # Train the model for one epoch\n",
    "    model.train()\n",
    "    for i in range(0, train_x.shape[0], batch_size):\n",
    "        # Get the current batch of training data\n",
    "        x = train_x[i:i+batch_size].reshape(-1, 1, 2, 1)\n",
    "        y = train_y[i:i+batch_size]\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db26e911-94c0-49b9-a3c5-c193883a73bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(df.drop(\"class\", axis=1), df[\"class\"], test_size=0.5, random_state=42)\n",
    "# X_train = torch.from_numpy(X_train.to_numpy().reshape(-1, 1, 3, 3)).float()\n",
    "# y_train = torch.from_numpy(y_train.to_numpy()).float()\n",
    "# X_test = torch.from_numpy(X_test.to_numpy().reshape(-1, 1, 3, 3)).float()\n",
    "# y_test = torch.from_numpy(y_test.to_numpy()).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fefa054-a6a8-4233-a3af-aebbc014bbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb98414-9ec7-43c6-8709-da204991a24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}, loss: {running_loss/len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1878716d-4e8c-436a-b33b-17c27c8aa413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up training parameters\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "# define the loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# train the model\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        # inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print('[Epoch %d] Loss: %.3f' % (epoch + 1, running_loss / len(train_loader)))\n",
    "\n",
    "# predict on the test set\n",
    "model.eval()\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        y_pred.append(outputs.detach().cpu().numpy().flatten())\n",
    "y_pred = np.concatenate(y_pred)\n",
    "\n",
    "# compute power function\n",
    "tau_0 = np.mean(y_test[y_test == 0])\n",
    "tau_1 = np.mean(y_test[y_test == 1])\n",
    "n0 = len(y_test[y_test == 0])\n",
    "n1 = len(y_test[y_test == 1])\n",
    "sigma_mx_2 = get_sigma_mx2(n0, n1, y_test, y_pred)\n",
    "se = np.sqrt(tau_1*(1-tau_1)/n1 + tau_0*(1-tau_0)/n0)\n",
    "power = norm.cdf((norm.ppf(0.8) - se*np.sqrt(sigma_mx_2.sum()))/np.sqrt(sigma_mx_2.sum()))\n",
    "print('Power:', power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e965ca-a40b-4d20-a44c-ca7b5bf465f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "191ec752323c442db01b072a853b2dd7b367cc1eb2408ac8b5045f825f91cc90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
