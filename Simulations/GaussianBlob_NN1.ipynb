{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63c1723d-b1e2-40ec-a6de-3f716ee13103",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c3e8153",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e010cb80-1a23-4384-b841-106b27d208ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch \n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "from utils import c2st_nn_fit, MatConvert, c2st_nn_power\n",
    "from models import myLDA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "424bd277-9088-46df-a5e8-d60f464f4c08",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b116ad3e-f404-478d-96c2-1fbb151ad802",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_mx2_standard = np.array([[0.03, 0], [0, 0.03]])\n",
    "sigma_mx2 = np.zeros((9, 2, 2))\n",
    "for i in range(9):\n",
    "    sigma_mx2[i, :, :] = sigma_mx2_standard\n",
    "    if i < 4:\n",
    "        sigma_mx2[i, 0, 1] = -0.02 - 0.002*i\n",
    "        sigma_mx2[i, 1, 0] = -0.02 - 0.002*i\n",
    "    elif i > 4:\n",
    "        sigma_mx2[i, 0, 1] = -0.02 + 0.002*(i-5)\n",
    "        sigma_mx2[i, 1, 0] = -0.02 + 0.002*(i-5)\n",
    "        \n",
    "sigma_mx2[4, :, :] = sigma_mx2_standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dfb3293-9e6a-4efc-8fd0-49349a2cf4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_blobs_Q(N0, N1, sigma_mx_2, rows=3, cols=3, rs=None):\n",
    "    \"\"\"Generate Blob-D for testing type-II error (or test power).\"\"\"\n",
    "    rs = check_random_state(rs)\n",
    "    mu = np.zeros(2)\n",
    "    sigma = np.eye(2) * 0.03\n",
    "    X = rs.multivariate_normal(mu, sigma, size=N0)\n",
    "    Y = rs.multivariate_normal(mu, np.eye(2), size=N1)\n",
    "    # assign to blobs\n",
    "    X[:, 0] += rs.randint(rows, size=N0)\n",
    "    X[:, 1] += rs.randint(cols, size=N0)\n",
    "    Y_row = rs.randint(rows, size=N1)\n",
    "    Y_col = rs.randint(cols, size=N1)\n",
    "    locs = [[0,0],[0,1],[0,2],[1,0],[1,1],[1,2],[2,0],[2,1],[2,2]]\n",
    "    for i in range(9):\n",
    "        corr_sigma = sigma_mx_2[i]\n",
    "        L = np.linalg.cholesky(corr_sigma)\n",
    "        ind = np.expand_dims((Y_row == locs[i][0]) & (Y_col == locs[i][1]), 1)\n",
    "        ind2 = np.concatenate((ind, ind), 1)\n",
    "        Y = np.where(ind2, np.matmul(Y,L) + locs[i], Y)\n",
    "    return X, Y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34c5000b",
   "metadata": {},
   "source": [
    "### Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be6c86b7-3907-4ddc-ae55-dde59ac81cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup seeds \n",
    "np.random.seed(1203)\n",
    "torch.manual_seed(1203)\n",
    "torch.cuda.manual_seed(1203)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "is_cuda = True\n",
    "\n",
    "# Setup for all experiments \n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda:0\")\n",
    "alpha = 0.05 # test threshold \n",
    "ir_list = [1,3,5,7,9] # imbalance ratio\n",
    "x_in = 2 # number of neurons in the input layer (dimension of data)\n",
    "H = 50 # number of neurons in the hidden layer\n",
    "x_out = 2 # number of neurons in the output layer \n",
    "K = 1 # number of experiments\n",
    "n1 = 100 # size of minority samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fc2f84a-200f-45a2-a339-c74690593acf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "\t\t\t\tImbalance Ratio: 1\n",
      "\t\t\t\tExperiments1\n",
      "====================================================================================================\n",
      "Epochs: 0\tTraining Binary CE Loss: 0.7004\tValidation Binary CE Loss: 0.7016\tTraining Accuracy: 0.5\tValidation Accuracy: 0.5\n",
      "Epochs: 100\tTraining Binary CE Loss: 0.6885\tValidation Binary CE Loss: 0.7042\tTraining Accuracy: 0.5267\tValidation Accuracy: 0.4667\n",
      "Epochs: 200\tTraining Binary CE Loss: 0.6776\tValidation Binary CE Loss: 0.7262\tTraining Accuracy: 0.5533\tValidation Accuracy: 0.4867\n",
      "Epochs: 300\tTraining Binary CE Loss: 0.662\tValidation Binary CE Loss: 0.7095\tTraining Accuracy: 0.5378\tValidation Accuracy: 0.5222\n",
      "Epochs: 400\tTraining Binary CE Loss: 0.6324\tValidation Binary CE Loss: 0.7318\tTraining Accuracy: 0.5911\tValidation Accuracy: 0.5489\n",
      "Epochs: 500\tTraining Binary CE Loss: 0.5995\tValidation Binary CE Loss: 0.7732\tTraining Accuracy: 0.6444\tValidation Accuracy: 0.5378\n",
      "Epochs: 600\tTraining Binary CE Loss: 0.5681\tValidation Binary CE Loss: 0.8741\tTraining Accuracy: 0.6667\tValidation Accuracy: 0.5222\n",
      "Epochs: 700\tTraining Binary CE Loss: 0.5478\tValidation Binary CE Loss: 1.2143\tTraining Accuracy: 0.6778\tValidation Accuracy: 0.5289\n",
      "Epochs: 800\tTraining Binary CE Loss: 0.5311\tValidation Binary CE Loss: 1.3451\tTraining Accuracy: 0.6889\tValidation Accuracy: 0.5378\n",
      "Epochs: 900\tTraining Binary CE Loss: 0.5162\tValidation Binary CE Loss: 1.9012\tTraining Accuracy: 0.7089\tValidation Accuracy: 0.5289\n",
      "Epochs: 1000\tTraining Binary CE Loss: 0.5328\tValidation Binary CE Loss: 2.0331\tTraining Accuracy: 0.7044\tValidation Accuracy: 0.5444\n",
      "Epochs: 1100\tTraining Binary CE Loss: 0.4852\tValidation Binary CE Loss: 2.475\tTraining Accuracy: 0.7289\tValidation Accuracy: 0.5333\n",
      "Epochs: 1200\tTraining Binary CE Loss: 0.4474\tValidation Binary CE Loss: 2.9811\tTraining Accuracy: 0.7667\tValidation Accuracy: 0.54\n",
      "acc_opt_cut:  0.8210000000000001 \t pwr_opt_cut 0.8210000000000001\n",
      "maximum power of acc_opt_cut:,  3.6121087074279785 \t maximum power of pwr_opt_cut:  3.6121087074279785\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\t\t\t\t   Testing\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epochs: 0\tTraining Binary CE Loss: 0.7501\tTraining Accuracy: 0.5\n",
      "Epochs: 100\tTraining Binary CE Loss: 0.691\tTraining Accuracy: 0.5211\n",
      "Epochs: 200\tTraining Binary CE Loss: 0.6906\tTraining Accuracy: 0.5267\n",
      "Epochs: 300\tTraining Binary CE Loss: 0.6902\tTraining Accuracy: 0.5278\n",
      "Epochs: 400\tTraining Binary CE Loss: 0.6895\tTraining Accuracy: 0.5422\n",
      "Epochs: 500\tTraining Binary CE Loss: 0.6833\tTraining Accuracy: 0.5511\n",
      "Epochs: 600\tTraining Binary CE Loss: 0.675\tTraining Accuracy: 0.5667\n",
      "Epochs: 700\tTraining Binary CE Loss: 0.6594\tTraining Accuracy: 0.5956\n",
      "Epochs: 800\tTraining Binary CE Loss: 0.6483\tTraining Accuracy: 0.6133\n",
      "Epochs: 900\tTraining Binary CE Loss: 0.6466\tTraining Accuracy: 0.5911\n",
      "Epochs: 1000\tTraining Binary CE Loss: 0.6343\tTraining Accuracy: 0.6256\n",
      "Epochs: 1100\tTraining Binary CE Loss: 0.6251\tTraining Accuracy: 0.6356\n",
      "Epochs: 1200\tTraining Binary CE Loss: 0.6175\tTraining Accuracy: 0.6389\n",
      "Epochs: 1300\tTraining Binary CE Loss: 0.6086\tTraining Accuracy: 0.6367\n",
      "Epochs: 1400\tTraining Binary CE Loss: 0.6008\tTraining Accuracy: 0.6433\n",
      "Epochs: 1500\tTraining Binary CE Loss: 0.592\tTraining Accuracy: 0.6467\n",
      "Epochs: 1600\tTraining Binary CE Loss: 0.5787\tTraining Accuracy: 0.6656\n",
      "Epochs: 1700\tTraining Binary CE Loss: 0.5634\tTraining Accuracy: 0.6822\n",
      "Epochs: 1800\tTraining Binary CE Loss: 0.5486\tTraining Accuracy: 0.6856\n",
      "Epochs: 1900\tTraining Binary CE Loss: 0.5299\tTraining Accuracy: 0.6956\n",
      "Epochs: 2000\tTraining Binary CE Loss: 0.5256\tTraining Accuracy: 0.7144\n",
      "Epochs: 2100\tTraining Binary CE Loss: 0.5052\tTraining Accuracy: 0.7189\n",
      "Epochs: 2200\tTraining Binary CE Loss: 0.4925\tTraining Accuracy: 0.73\n",
      "Epochs: 2300\tTraining Binary CE Loss: 0.4955\tTraining Accuracy: 0.7278\n",
      "Epochs: 2400\tTraining Binary CE Loss: 0.4697\tTraining Accuracy: 0.7456\n",
      "Statistics of maximizing acc: 3.872396469116211 \t tau0_a: 0.795555591583252 \t tau1_a: 0.1111111119389534\n",
      "Statistics of default: 4.176539897918701 \t tau0_d: 0.4511111080646515 \t tau1_d: 0.41111111640930176\n",
      "Statistics of maximizing pwr: 3.872396469116211 \t tau0_p: 0.795555591583252 \t tau1_p: 0.1111111119389534\n",
      "====================================================================================================\n",
      "\t\t\t\tImbalance Ratio: 1\n",
      "\t\t\t\tExperiments2\n",
      "====================================================================================================\n",
      "Epochs: 0\tTraining Binary CE Loss: 0.7016\tValidation Binary CE Loss: 0.7011\tTraining Accuracy: 0.5\tValidation Accuracy: 0.5\n",
      "Epochs: 100\tTraining Binary CE Loss: 0.6801\tValidation Binary CE Loss: 0.6964\tTraining Accuracy: 0.5533\tValidation Accuracy: 0.4933\n",
      "Epochs: 200\tTraining Binary CE Loss: 0.6536\tValidation Binary CE Loss: 0.7116\tTraining Accuracy: 0.58\tValidation Accuracy: 0.5089\n",
      "Epochs: 300\tTraining Binary CE Loss: 0.6276\tValidation Binary CE Loss: 0.7105\tTraining Accuracy: 0.6044\tValidation Accuracy: 0.5289\n",
      "Epochs: 400\tTraining Binary CE Loss: 0.6\tValidation Binary CE Loss: 0.7641\tTraining Accuracy: 0.62\tValidation Accuracy: 0.5311\n",
      "Epochs: 500\tTraining Binary CE Loss: 0.602\tValidation Binary CE Loss: 0.9254\tTraining Accuracy: 0.6311\tValidation Accuracy: 0.5533\n",
      "Epochs: 600\tTraining Binary CE Loss: 0.5402\tValidation Binary CE Loss: 1.219\tTraining Accuracy: 0.6667\tValidation Accuracy: 0.5556\n",
      "Epochs: 700\tTraining Binary CE Loss: 0.5189\tValidation Binary CE Loss: 1.6509\tTraining Accuracy: 0.6844\tValidation Accuracy: 0.5533\n",
      "Epochs: 800\tTraining Binary CE Loss: 0.4861\tValidation Binary CE Loss: 2.2652\tTraining Accuracy: 0.7067\tValidation Accuracy: 0.5244\n",
      "Epochs: 900\tTraining Binary CE Loss: 0.4667\tValidation Binary CE Loss: 2.8741\tTraining Accuracy: 0.72\tValidation Accuracy: 0.5467\n",
      "Epochs: 1000\tTraining Binary CE Loss: 0.4562\tValidation Binary CE Loss: 3.6928\tTraining Accuracy: 0.7311\tValidation Accuracy: 0.5578\n",
      "Epochs: 1100\tTraining Binary CE Loss: 0.4689\tValidation Binary CE Loss: 3.2459\tTraining Accuracy: 0.6911\tValidation Accuracy: 0.5644\n",
      "Epochs: 1200\tTraining Binary CE Loss: 0.455\tValidation Binary CE Loss: 4.1545\tTraining Accuracy: 0.7\tValidation Accuracy: 0.5489\n",
      "acc_opt_cut:  0.495 \t pwr_opt_cut 0.56\n",
      "maximum power of acc_opt_cut:,  2.767230272293091 \t maximum power of pwr_opt_cut:  3.0198659896850586\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\t\t\t\t   Testing\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epochs: 0\tTraining Binary CE Loss: 0.7491\tTraining Accuracy: 0.5\n",
      "Epochs: 100\tTraining Binary CE Loss: 0.6917\tTraining Accuracy: 0.5267\n",
      "Epochs: 200\tTraining Binary CE Loss: 0.6901\tTraining Accuracy: 0.5344\n",
      "Epochs: 300\tTraining Binary CE Loss: 0.6839\tTraining Accuracy: 0.5422\n",
      "Epochs: 400\tTraining Binary CE Loss: 0.6693\tTraining Accuracy: 0.5622\n",
      "Epochs: 500\tTraining Binary CE Loss: 0.6671\tTraining Accuracy: 0.5656\n",
      "Epochs: 600\tTraining Binary CE Loss: 0.6663\tTraining Accuracy: 0.5656\n",
      "Epochs: 700\tTraining Binary CE Loss: 0.6654\tTraining Accuracy: 0.5656\n",
      "Epochs: 800\tTraining Binary CE Loss: 0.663\tTraining Accuracy: 0.5733\n",
      "Epochs: 900\tTraining Binary CE Loss: 0.6576\tTraining Accuracy: 0.5678\n",
      "Epochs: 1000\tTraining Binary CE Loss: 0.6519\tTraining Accuracy: 0.5867\n",
      "Epochs: 1100\tTraining Binary CE Loss: 0.6482\tTraining Accuracy: 0.5911\n",
      "Epochs: 1200\tTraining Binary CE Loss: 0.6415\tTraining Accuracy: 0.6033\n",
      "Epochs: 1300\tTraining Binary CE Loss: 0.6367\tTraining Accuracy: 0.6156\n",
      "Epochs: 1400\tTraining Binary CE Loss: 0.6329\tTraining Accuracy: 0.6122\n",
      "Epochs: 1500\tTraining Binary CE Loss: 0.6171\tTraining Accuracy: 0.6267\n",
      "Epochs: 1600\tTraining Binary CE Loss: 0.6104\tTraining Accuracy: 0.6367\n",
      "Epochs: 1700\tTraining Binary CE Loss: 0.5997\tTraining Accuracy: 0.6456\n",
      "Epochs: 1800\tTraining Binary CE Loss: 0.5877\tTraining Accuracy: 0.6533\n",
      "Epochs: 1900\tTraining Binary CE Loss: 0.5876\tTraining Accuracy: 0.6578\n",
      "Epochs: 2000\tTraining Binary CE Loss: 0.5733\tTraining Accuracy: 0.6611\n",
      "Epochs: 2100\tTraining Binary CE Loss: 0.569\tTraining Accuracy: 0.6767\n",
      "Epochs: 2200\tTraining Binary CE Loss: 0.5628\tTraining Accuracy: 0.6756\n",
      "Epochs: 2300\tTraining Binary CE Loss: 0.5518\tTraining Accuracy: 0.6822\n",
      "Epochs: 2400\tTraining Binary CE Loss: 0.5462\tTraining Accuracy: 0.69\n",
      "Statistics of maximizing acc: 4.879241943359375 \t tau0_a: 0.5133333206176758 \t tau1_a: 0.3288888931274414\n",
      "Statistics of default: 5.107511043548584 \t tau0_d: 0.5177778005599976 \t tau1_d: 0.3177777826786041\n",
      "Statistics of maximizing pwr: 6.086125373840332 \t tau0_p: 0.6088889241218567 \t tau1_p: 0.20888888835906982\n",
      "====================================================================================================\n",
      "\t\t\t\tImbalance Ratio: 1\n",
      "\t\t\t\tExperiments3\n",
      "====================================================================================================\n",
      "Epochs: 0\tTraining Binary CE Loss: 0.7009\tValidation Binary CE Loss: 0.7007\tTraining Accuracy: 0.5\tValidation Accuracy: 0.5\n",
      "Epochs: 100\tTraining Binary CE Loss: 0.6823\tValidation Binary CE Loss: 0.7001\tTraining Accuracy: 0.5578\tValidation Accuracy: 0.5\n",
      "Epochs: 200\tTraining Binary CE Loss: 0.6562\tValidation Binary CE Loss: 0.7126\tTraining Accuracy: 0.5933\tValidation Accuracy: 0.5133\n",
      "Epochs: 300\tTraining Binary CE Loss: 0.5987\tValidation Binary CE Loss: 0.7842\tTraining Accuracy: 0.6711\tValidation Accuracy: 0.5333\n",
      "Epochs: 400\tTraining Binary CE Loss: 0.5523\tValidation Binary CE Loss: 1.1054\tTraining Accuracy: 0.6933\tValidation Accuracy: 0.5733\n",
      "Epochs: 500\tTraining Binary CE Loss: 0.5119\tValidation Binary CE Loss: 1.1912\tTraining Accuracy: 0.7\tValidation Accuracy: 0.5578\n",
      "Epochs: 600\tTraining Binary CE Loss: 0.484\tValidation Binary CE Loss: 1.2664\tTraining Accuracy: 0.7467\tValidation Accuracy: 0.5333\n",
      "Epochs: 700\tTraining Binary CE Loss: 0.4596\tValidation Binary CE Loss: 1.5112\tTraining Accuracy: 0.7644\tValidation Accuracy: 0.5578\n",
      "Epochs: 800\tTraining Binary CE Loss: 0.422\tValidation Binary CE Loss: 1.841\tTraining Accuracy: 0.7689\tValidation Accuracy: 0.5622\n",
      "Epochs: 900\tTraining Binary CE Loss: 0.4008\tValidation Binary CE Loss: 2.0249\tTraining Accuracy: 0.78\tValidation Accuracy: 0.5733\n",
      "Epochs: 1000\tTraining Binary CE Loss: 0.3697\tValidation Binary CE Loss: 2.1875\tTraining Accuracy: 0.8089\tValidation Accuracy: 0.5711\n",
      "Epochs: 1100\tTraining Binary CE Loss: 0.3498\tValidation Binary CE Loss: 2.8934\tTraining Accuracy: 0.8156\tValidation Accuracy: 0.5622\n",
      "Epochs: 1200\tTraining Binary CE Loss: 0.3477\tValidation Binary CE Loss: 2.4078\tTraining Accuracy: 0.8089\tValidation Accuracy: 0.5756\n",
      "acc_opt_cut:  0.47600000000000003 \t pwr_opt_cut 0.47600000000000003\n",
      "maximum power of acc_opt_cut:,  3.4510598182678223 \t maximum power of pwr_opt_cut:  3.4510598182678223\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\t\t\t\t   Testing\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epochs: 0\tTraining Binary CE Loss: 0.7499\tTraining Accuracy: 0.5\n",
      "Epochs: 100\tTraining Binary CE Loss: 0.6932\tTraining Accuracy: 0.5011\n",
      "Epochs: 200\tTraining Binary CE Loss: 0.6931\tTraining Accuracy: 0.4833\n",
      "Epochs: 300\tTraining Binary CE Loss: 0.6928\tTraining Accuracy: 0.4956\n",
      "Epochs: 400\tTraining Binary CE Loss: 0.6758\tTraining Accuracy: 0.5522\n",
      "Epochs: 500\tTraining Binary CE Loss: 0.6695\tTraining Accuracy: 0.5511\n",
      "Epochs: 600\tTraining Binary CE Loss: 0.6589\tTraining Accuracy: 0.5789\n",
      "Epochs: 700\tTraining Binary CE Loss: 0.6455\tTraining Accuracy: 0.5967\n",
      "Epochs: 800\tTraining Binary CE Loss: 0.6155\tTraining Accuracy: 0.6467\n",
      "Epochs: 900\tTraining Binary CE Loss: 0.5929\tTraining Accuracy: 0.6711\n",
      "Epochs: 1000\tTraining Binary CE Loss: 0.569\tTraining Accuracy: 0.6767\n",
      "Epochs: 1100\tTraining Binary CE Loss: 0.5577\tTraining Accuracy: 0.6811\n",
      "Epochs: 1200\tTraining Binary CE Loss: 0.5444\tTraining Accuracy: 0.6956\n",
      "Epochs: 1300\tTraining Binary CE Loss: 0.5504\tTraining Accuracy: 0.6789\n",
      "Epochs: 1400\tTraining Binary CE Loss: 0.5204\tTraining Accuracy: 0.7167\n",
      "Epochs: 1500\tTraining Binary CE Loss: 0.5121\tTraining Accuracy: 0.7189\n",
      "Epochs: 1600\tTraining Binary CE Loss: 0.4953\tTraining Accuracy: 0.7378\n",
      "Epochs: 1700\tTraining Binary CE Loss: 0.4931\tTraining Accuracy: 0.7356\n"
     ]
    }
   ],
   "source": [
    "# For each n in n_list, train deep kernel and run two-sample test\n",
    "stats_a_dict = {}\n",
    "stats_d_dict = {}\n",
    "stats_p_dict = {}\n",
    "\n",
    "acc_opt_cuts = []\n",
    "pwr_opt_cuts = []\n",
    "\n",
    "a_powers = {}\n",
    "d_powers = {}\n",
    "p_powers = {}\n",
    "\n",
    "for r in ir_list:\n",
    "    np.random.seed(1203)\n",
    "    N1 = 9 * n1 * r\n",
    "    N2 = 9 * n1\n",
    "    batch_size = N1 + N2 # not using batch \n",
    "    n_epoch_c2st = 1200\n",
    "    \n",
    "    # Repeat experiments K times (K = 10) and report average test powers\n",
    "    stats_a_nn = []\n",
    "    stats_d_nn = []\n",
    "    stats_p_nn = []\n",
    "    \n",
    "    for kk in range(50):\n",
    "        print('='*100)\n",
    "        print(\"\\t\\t\\t\\tImbalance Ratio: {}\".format(int(N1/N2)))\n",
    "        print(f\"\\t\\t\\t\\tExperiments{kk+1}\")\n",
    "        print('='*100)\n",
    "        # Generate Blob-D\n",
    "        np.random.seed(seed=112*kk + 1 + N1)\n",
    "        s1, s2 = sample_blobs_Q(N1, N2, sigma_mx2)\n",
    "        \n",
    "        S = np.concatenate((s1,s2), axis=0)\n",
    "        S = MatConvert(S, device, dtype)\n",
    "        \n",
    "        # Train C2ST-L\n",
    "        np.random.seed(seed=1203)\n",
    "        torch.manual_seed(1203)\n",
    "        torch.cuda.manual_seed(1203)\n",
    "        y = (torch.cat((torch.zeros(N1, 1), torch.ones(N2, 1)), 0)).squeeze(1).to(device, dtype).long()\n",
    "        cutvals = np.arange(start=0.001, stop=0.999, step = 0.001)\n",
    "        acc, pwr, acc_opt_cut, pwr_opt_cut = c2st_nn_fit(S, y, x_in, H, x_out, cutvals, 0.01, n_epoch_c2st, batch_size, device, dtype, validation=True)\n",
    "        acc_opt_cuts.append(acc_opt_cut)\n",
    "        pwr_opt_cuts.append(pwr_opt_cut)\n",
    "        print(\"-\"*100)\n",
    "        print('-'*100)\n",
    "        print(\"\\t\\t\\t\\t   Testing\")\n",
    "        print('-'*100)\n",
    "        print('-'*100)\n",
    "        tau0_a, tau1_a, tau0_p, tau1_p, stat_a, stat_d, stat_p = c2st_nn_fit(S, y, x_in, H, x_out, cutvals, 0.01, n_epoch_c2st*2, batch_size, device, dtype, validation=False, acc_opt_cut=acc_opt_cut, pwr_opt_cut=pwr_opt_cut)\n",
    "        \n",
    "        # statistics\n",
    "        stats_a_nn.append(stat_a)\n",
    "        stats_d_nn.append(stat_d)\n",
    "        stats_p_nn.append(stat_p)\n",
    "\n",
    "    stats_a_dict[r] = stats_a_nn\n",
    "    stats_d_dict[r] = stats_d_nn\n",
    "    stats_p_dict[r] = stats_p_nn\n",
    "    # power\n",
    "    cr = norm.ppf(1-alpha)\n",
    "    a_pwr = np.mean((stats_a_nn > (1-norm.ppf(alpha))))\n",
    "    d_pwr = np.mean((stats_d_nn > (1-norm.ppf(alpha))))\n",
    "    p_pwr = np.mean((stats_p_nn > (1-norm.ppf(alpha))))\n",
    "    \n",
    "    a_powers[r] = a_pwr\n",
    "    d_powers[r] = d_pwr\n",
    "    p_powers[r] = p_pwr\n",
    "    print(\"Accuracy Power: {}\".format(a_pwr), '\\t', \"Default Power: {}\".format(d_pwr), '\\t', \"Power Power: {}\".format(p_pwr))\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d57403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each n in n_list, train deep kernel and run two-sample test\n",
    "stats_a_dict = {}\n",
    "stats_d_dict = {}\n",
    "stats_p_dict = {}\n",
    "\n",
    "acc_opt_cuts = []\n",
    "pwr_opt_cuts = []\n",
    "\n",
    "a_powers = {}\n",
    "d_powers = {}\n",
    "p_powers = {}\n",
    "\n",
    "for r in ir_list:\n",
    "    np.random.seed(1203)\n",
    "    N1 = n1 * r\n",
    "    N2 = n1\n",
    "    batch_size = N1 + N2 # not using batch \n",
    "    n_epoch_c2st = 1000\n",
    "    \n",
    "    # Repeat experiments K times (K = 10) and report average test powers\n",
    "    stats_a = []\n",
    "    stats_d = []\n",
    "    stats_p = []\n",
    "    \n",
    "    for kk in range(K):\n",
    "        print('='*100)\n",
    "        print(\"\\t\\t\\t\\tImbalance Ratio: {}\".format(int(N1/N2)))\n",
    "        print(f\"\\t\\t\\t\\tExperiments{kk+1}\")\n",
    "        print('='*100)\n",
    "        # Generate Blob-D\n",
    "        np.random.seed(seed=112*kk + 1 + N1)\n",
    "        s1, s2 = sample_blobs_Q(N1, N2, sigma_mx2)\n",
    "        \n",
    "        if kk == 0:\n",
    "            s1_o = s1\n",
    "            s2_o = s2\n",
    "        \n",
    "        S = np.concatenate((s1,s2), axis=0)\n",
    "        S = MatConvert(S, device, dtype)\n",
    "        \n",
    "        # Train C2ST-L\n",
    "        np.random.seed(seed=1203)\n",
    "        torch.manual_seed(1203)\n",
    "        torch.cuda.manual_seed(1203)\n",
    "        y = (torch.cat((torch.zeros(N1, 1), torch.ones(N2, 1)), 0)).squeeze(1).to(device, dtype).long()\n",
    "        cutvals = np.linspace(0.001, 0.999, 1000)\n",
    "        pred, tau0, tau1, stat_c2st = c2st_nn_power(S, y, x_in, H, x_out, 0.01, n_epoch_c2st, batch_size, device, dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cac6a7-876d-4a02-a1c6-c4ac62799b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each n in n_list, train deep kernel and run two-sample test\n",
    "\n",
    "stats = {}\n",
    "powers = []\n",
    "for n in n_list:\n",
    "    np.random.seed(12203)\n",
    "    torch.manual_seed(12203)\n",
    "    torch.cuda.manual_seed(12203)\n",
    "    N1 = 30 * n\n",
    "    N2 = 300\n",
    "    results = np.zeros([5, K])\n",
    "    J_star_adp = np.zeros([K, n_epoch])\n",
    "    batch_size = N1 + N2 \n",
    "    n_epoch_c2st = 500\n",
    "    \n",
    "    # Repeat experiments K times (K = 10) and report average test power \n",
    "    stat = []\n",
    "    for kk in range(K):\n",
    "        print(\"Imbalance Ratio: {}\".format(N1/N2))\n",
    "        print(f\"{kk+1} Experiments...\")\n",
    "        \n",
    "        # Generate Blob-D\n",
    "        np.random.seed(seed=112*kk + 1 + n)\n",
    "        s1, s2 = sample_blobs_Q(N1, N2, sigma_mx2)\n",
    "        \n",
    "        if kk == 0:\n",
    "            s1_o = s1\n",
    "            s2_o = s2\n",
    "        \n",
    "        S = np.concatenate((s1,s2), axis=0)\n",
    "        S = MatConvert(S, device, dtype)\n",
    "        \n",
    "        # Train C2ST-L\n",
    "        np.random.seed(seed=12203)\n",
    "        torch.manual_seed(12203)\n",
    "        torch.cuda.manual_seed(12203)\n",
    "        y = (torch.cat((torch.zeros(N1, 1), torch.ones(N2, 1)), 0)).squeeze(1).to(device, dtype).long()\n",
    "        pred, tau0, tau1, stat_c2st_l, model_c2st_l, w_c2st_l, b_c2st_l = C2ST_NN_power(S, y, N1, x_in, H, x_out, 0.0005, \n",
    "                                                                          n_epoch_c2st, batch_size, device, dtype)\n",
    "        # statistics\n",
    "        stat.append(stat_c2st_l)\n",
    "        print(pred.unique(return_counts=True))\n",
    "        print(f\"tau0: {tau0} \\t tau1: {tau1} \", end='\\t')\n",
    "        print(\"Stats: {}\".format(stat_c2st_l))\n",
    "        stats[n] = stat\n",
    "        \n",
    "    # power\n",
    "    pwr = len([i for i in stats[n] if i > (1-norm.ppf(alpha))]) / len(stats[n])\n",
    "    powers.append(pwr)\n",
    "    print(\"Power: {}\".format(pwr))\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eaad4c-d1e0-4180-8b8d-e59c71752385",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5bd4db-270b-45cb-96d6-3f95c5764b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901bfb54-fe75-4f3c-bb69-17a959422e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_c2st_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6827a62-0b4f-4121-843d-5df6fc5340e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.unique(return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75660d39-8982-409b-9bee-bc0fe74a7699",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1999ee15-f2fa-4651-8a1b-86f3023a7f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "df = sample_blobs_Q(500, 500, sigma_mx2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f940a2-5f2b-4f00-a3c2-474c9712f948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "data = sample_blobs_Q(n0=500, n1=500, sigma_mx_2=sigma_mx2)\n",
    "\n",
    "# Split data into training and test sets\n",
    "train_data = data.sample(frac=0.8, random_state=42)\n",
    "test_data = data.drop(train_data.index)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "train_x = torch.tensor(train_data.iloc[:, :-1].values, dtype=torch.float32)\n",
    "train_y = torch.tensor(train_data.iloc[:, -1].values, dtype=torch.int64)\n",
    "test_x = torch.tensor(test_data.iloc[:, :-1].values, dtype=torch.float32)\n",
    "test_y = torch.tensor(test_data.iloc[:, -1].values, dtype=torch.int64)\n",
    "\n",
    "# Define the LeNet model\n",
    "model = LeNet()\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    # Shuffle the training data\n",
    "    indices = torch.randperm(train_x.shape[0])\n",
    "    train_x = train_x[indices]\n",
    "    train_y = train_y[indices]\n",
    "    \n",
    "    # Train the model for one epoch\n",
    "    model.train()\n",
    "    for i in range(0, train_x.shape[0], batch_size):\n",
    "        # Get the current batch of training data\n",
    "        x = train_x[i:i+batch_size].reshape(-1, 1, 2, 1)\n",
    "        y = train_y[i:i+batch_size]\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db26e911-94c0-49b9-a3c5-c193883a73bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(df.drop(\"class\", axis=1), df[\"class\"], test_size=0.5, random_state=42)\n",
    "# X_train = torch.from_numpy(X_train.to_numpy().reshape(-1, 1, 3, 3)).float()\n",
    "# y_train = torch.from_numpy(y_train.to_numpy()).float()\n",
    "# X_test = torch.from_numpy(X_test.to_numpy().reshape(-1, 1, 3, 3)).float()\n",
    "# y_test = torch.from_numpy(y_test.to_numpy()).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fefa054-a6a8-4233-a3af-aebbc014bbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb98414-9ec7-43c6-8709-da204991a24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}, loss: {running_loss/len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1878716d-4e8c-436a-b33b-17c27c8aa413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up training parameters\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "# define the loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# train the model\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        # inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print('[Epoch %d] Loss: %.3f' % (epoch + 1, running_loss / len(train_loader)))\n",
    "\n",
    "# predict on the test set\n",
    "model.eval()\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        y_pred.append(outputs.detach().cpu().numpy().flatten())\n",
    "y_pred = np.concatenate(y_pred)\n",
    "\n",
    "# compute power function\n",
    "tau_0 = np.mean(y_test[y_test == 0])\n",
    "tau_1 = np.mean(y_test[y_test == 1])\n",
    "n0 = len(y_test[y_test == 0])\n",
    "n1 = len(y_test[y_test == 1])\n",
    "sigma_mx_2 = get_sigma_mx2(n0, n1, y_test, y_pred)\n",
    "se = np.sqrt(tau_1*(1-tau_1)/n1 + tau_0*(1-tau_0)/n0)\n",
    "power = norm.cdf((norm.ppf(0.8) - se*np.sqrt(sigma_mx_2.sum()))/np.sqrt(sigma_mx_2.sum()))\n",
    "print('Power:', power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e965ca-a40b-4d20-a44c-ca7b5bf465f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "191ec752323c442db01b072a853b2dd7b367cc1eb2408ac8b5045f825f91cc90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
